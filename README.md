1. Выбранная задача

Цель работы — оценить эффективность трёх типов генеративных моделей (текст, изображение, речь) с применением стандартных метрик. Оценка проводилась по следующим критериям:

Текстовые модели — BLEU и ROUGE

Изобразительные модели — FID (Fréchet Inception Distance)

Модели речи — STOI (Short-Time Objective Intelligibility) и MOS (Mean Opinion Score, субъективная)

2. Реализация

2.1. Структура проекта

<pre>
genai_eval/
├── metrics/                 # Модули с вычислением метрик
│   ├── text_metrics.py      # BLEU, ROUGE
│   ├── image_metrics.py     # FID
│   └── speech_metrics.py    # STOI, MOS placeholder
├── scripts/                 # Генераторы тестовых данных
│   ├── generate_test_audio.py
│   └── generate_test_images.py
├── data/                    # Директории с данными
│   ├── images/
│   │   ├── real/
│   │   └── fake/
│   └── audio/
├── config.yaml              # Настройки путей и тестовых данных
├── main.py                  # CLI-скрипт для запуска метрик
├── Makefile                 # Быстрый запуск команд
├── requirements.txt
└── README.md
</pre>

2.2. Используемые библиотеки

Текст — nltk, rouge-score, sacrebleu

Изображения — torch, torchvision, pytorch-fid

Речь — pystoi, soundfile

Общее — PyYAML для конфигов


2.3. Основные функции оценки

Текст

compute_bleu(references, candidates)

compute_rouge(references, candidates)

Изображения

compute_fid(real_path, fake_path)

Речь

compute_stoi(ref_path, deg_path)

mos_placeholder()  # Комментарий о необходимости ручной оценки

2.4. Автоматизация

Запуск осуществляется через Makefile:

make eval_text

make eval_image

make eval_speech

Или напрямую:

python main.py --config config.yaml --task text

3. Результаты оценки

| Тип модели      | Метрика   | Значение | Интерпретация |
|-----------------|-----------|----------|---------------|
| **Текст**       | BLEU      | 47.77    | Среднее совпадение по n-граммам; перефразировки снижают балл |
|                 | ROUGE-1   | 0.96     | Почти полное совпадение по словам |
|                 | ROUGE-L   | 0.85     | Высокое совпадение по структуре предложений |
| **Изображения** | FID       | 34.06    | <50 — хорошее совпадение распределений признаков |
| **Речь**        | STOI      | 0.0151   | Низкая разборчивость (тестовые данные с шумом) |
|                 | MOS       | —        | Требует субъективной оценки слушателей |

4. Критерии оценки

* Правильность — Метрики отработали корректно, результаты логично совпадают с субъективной оценкой 		тестовых данных.

* Полнота — Покрыты базовые аспекты оценки: лексика/структура (текст), распределение признаков (изображения), разборчивость (речь).

* Эффективность — Автоматизация позволяет получить все метрики за секунды; легко интегрировать в CI/CD.

5. Выводы
Наиболее полезной для текста оказалась ROUGE-L — хорошо оценивает структурное совпадение даже при перефразировках.

BLEU информативен, но чувствителен к лексическим заменам.

FID подтвердил высокую схожесть изображений, что полезно для автоматического контроля качества.

STOI позволяет выявлять низкую разборчивость речи без субъективной оценки.

MOS автоматизировать невозможно, но его можно дополнять STOI/PESQ для комплексной проверки.

Комбинация метрик даёт более полную оценку качества моделей.

Планируется интеграция метрик в CI/CD для регулярного мониторинга.